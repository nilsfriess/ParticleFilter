\section*{Introduction}\label{sec:intro}
In this report we present an implementation of a Particle Filter in
C++. Before discussing the actual implementation we give the
theoretical details of the method in this section. We start by giving
a brief overview of the method before discussing the individual steps
in more detail.

We remark here, that the naming in these methods is highly ambiguous
and varies greatly from author to author. We use -- with some
exceptions -- the naming and notation used in~\cite{doucet} and
highlight parts where the naming differs from other publications.

\subsection*{Overview of a Particle Filter}
A Particle Filter is a Sequential Monte Carlo (SMC)
method\footnote{Some authors use the terms \emph{Particle Filter} and
  \emph{SMC method} synonymously. Doucet and Johansen develop
  in~\cite{doucet} a framework in which Particle Filters are only one
  specific method in the much broader class of SMC methods. They argue
  that this distinction allows for a better understanding of these
  methods. In this report, we are only interested in the filtering
  problem and will introduce it without discussing the more general
  notion of SMC methods as given by Doucet and Johansen.} that is used
to estimate the state of a system that changes over time using only
noisy and/ or partial observations of the system's state. This will be
done in a Bayesian framework where one attempts to construct the
posterior probability density function (pdf) of the state based on the
observations. We make the following assumptions:
\begin{itemize}
\item The model describing the evolution of the internal state in time
  is available in a probablistic form.
\item The model that relates the observations to the internal state is
  available in a probabilistic form.
\item The observations are only available sequentially, not as a batch
  (ie. we assume that we receive new measurments sequentially in
  time).
\end{itemize}
Due to the last assumption we aim at a recursive method that does
neither require to store nor to reprocess all the previous information
when a new observation becomes available. To formalise the first two
assumptions we will use the notion of \emph{hidden markov models}.

To that end, we consider an $\R^{d_x}$-valued discrete-time Markov
process ${\{X_n\}}_{n \in \N_0}$ such that
\begin{equation}
  \label{eq:hmm:1}
  X_0 \sim \mu(x_0) \qquad \text{and} \qquad X_n \mid (X_{n-1} = x_{n-1}) \sim f(x_n \mid x_{n-1})\,, \quad n \ge 1\,,
\end{equation}
where $f(x \mid x^\prime)$ denotes the probability density associated
with moving from state $x^\prime$ to $x$. This process models the
internal state that we try to estimate. However, only the
$\R^{d_y}$-valued process ${\{Y_n\}}_{n \in \N}$ is available. We
assume that these observations are statistically independent and that
\begin{equation}
  \label{eq:hmm:2}
  Y_n \mid (X_n = x_n) \sim g(y_n \mid x_n) \,.
\end{equation}
Our goal is now to estimate the state of the system at some time $n$
which can now be stated as: we want to estimate the distribution
$p(x_n \mid y_{1:n})$, where
$y_{1:n} \coloneqq (y_1, y_2, \dotsc, y_n)$. This is often referred to
as the \emph{filtering problem} or \emph{tracking}.\footnote{Note that
  Docuet and Johansen \emph{do not} call this the filtering
  problem~\cite{doucet}. They reserve this term for the estimation of
  the joint distributions $p(x_{1:n} \mid y_{1:n} )$. Since we are
  only concerned with estimating the marginal distribution
  $p(x_n \mid y_{1:n} )$ we will still refer to this problem as
  filtering.}

In a Bayesian framework this is achieved in two steps: prediction and
update. By the Chapman-Kolmogorov equation we first obtain
\begin{align*}
  p(x_{k+1} \mid y_{1:k}) &= \int p(x_{k+1} \mid x_k, y_{1:k}) p(x_k \mid y_{1:k}) \dx_k \\
                          &= \int f(x_{k+1} \mid x_k) p(x_k \mid y_{1:k}) \dx_k \,,
\end{align*}
where we used the Markov property of the system, \ie
\[
  p(x_{k+1} \mid x_k, y_{1:k}) = p(x_{k+1} \mid x_k) = f(x_{k+1} \mid
  x_k) \,.
\]
Once a new observation $y_{k+1}$ becomes available we can update our
beliefs about the system's state using Bayes' theorem
\[
  p(x_{k+1} \mid y_{1:k+1}) = \frac{ p(y_{k+1} \mid x_{k+1}) p(x_{k+1}
    \mid y_{1:k}) }{ p(y_{k+1} \mid y_{1:k}) }\,,
\]
where the normalising constant
\begin{align*}
  p(y_{k+1} \mid y_{1:k}) &= \int p(y_{k+1} \mid x_{k+1}) p(x_{k+1} \mid y_{1:k}) \dx_{k+1} \\
                          &= \int g(y_{k+1} \mid x_{k+1}) p(x_{k+1} \mid y_{1:k}) \dx_{k+1}
\end{align*}
depends on the likelihood function defined by the model
in~\eqref{eq:hmm:2}. Putting these two steps together, we obtain a
recursive formula using the previous filtered state of the system
$p(x_{k} \mid y_{1:k})$ and a new observation $y_{k+1}$ to compute the
current state of the system.

Only in a restrictive set of cases can the arising integrals be
computed in a closed-form (e.\,g.\ when $f$ and $g$ are linear and the
posterior of the system is Gaussian~\cite[175]{arulampalam} or when
the underlying state space of the Markov model is finite,
cf.~\cite[Example 1]{doucet}). In a more general nonlinear
non-Gaussian setting, approximative methods such as particle filters
are necessary.

\section*{Particle Filtering Methods}
The central idea of Particle filters is to represent the posterior
probability density function (pdf) as a weighted set of
\emph{particles}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
